{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import html\n",
    "from nltk.tokenize import casual_tokenize\n",
    "# from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo funções auxiliares\n",
    "\n",
    "# *************************************************************\n",
    "# Limpeza de texto\n",
    "# *************************************************************\n",
    "\n",
    "MD_LINK_RE   = re.compile(r'\\[([^\\]]+)\\]\\((?:https?://|www\\.)[^\\s)]+\\)', re.I)\n",
    "SCHEME_URL_RE= re.compile(r'(?i)\\b(?:https?://|http://|ftp://|www\\.)[^\\s<>()]+\\b')\n",
    "BARE_DOMAIN_RE = re.compile(\n",
    "    r'(?i)\\b(?!\\S+@)(?:[a-z0-9-]+\\.)+(?:[a-z]{2,})(?:/[^\\s<>()]*)?\\b'\n",
    ")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean HTML, emojis/odd symbols, control chars; tokenize with NLTK casual and join.\"\"\"\n",
    "    if text is None or (isinstance(text, float) and pd.isna(text)):\n",
    "        return \"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "        \n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    text = re.sub(r\"[\\r\\n\\t\\f\\v]+\", \" \", text)\n",
    "    \n",
    "    # removendo alguns links\n",
    "    text = MD_LINK_RE.sub(r\"\\1\", text)\n",
    "    text = SCHEME_URL_RE.sub(\" \", text)\n",
    "    text = BARE_DOMAIN_RE.sub(\" \", text)\n",
    "    \n",
    "    # removendo não alfanuméricos, exceto pontuação básica\n",
    "    text = re.sub(r\"[^0-9A-Za-zÀ-ÖØ-öø-ÿçÇ\\s\\.\\,\\!\\?\\:\\;\\'\\\"\\(\\)\\-\\_\\/]\", \" \", text)\n",
    "    \n",
    "    # tokenização com casual tokenizer\n",
    "    tokens = casual_tokenize(text, reduce_len=True, strip_handles=True, preserve_case=False)\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    # removendo espaços extras\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# *************************************************************\n",
    "# Criando arquivo de treinamento para spaCy\n",
    "# *************************************************************\n",
    "def find_tokens(input_filename, output_filename, num_rows_to_process):\n",
    "    df = pd.read_csv(input_filename)\n",
    "\n",
    "    # Lista de operadoras para procurar\n",
    "    operators = [\"vivo\", \"oi\", \"tim\", \"claro\"]\n",
    "    train_data = []\n",
    "\n",
    "    # Garante que não vamos processar mais linhas do que as que existem no arquivo.\n",
    "    actual_rows = min(num_rows_to_process, len(df))\n",
    "\n",
    "    for i in range(0, actual_rows):\n",
    "        \n",
    "        text = df.loc[i, \"text\"]\n",
    "        \n",
    "        # Check se o texto é uma string válida\n",
    "        if not isinstance(text, str):\n",
    "            continue\n",
    "\n",
    "        entities = []\n",
    "\n",
    "        for op in operators:\n",
    "            # Usa limites de palavra para encontrar apenas palavras inteiras, ignorando o case\n",
    "            for match in re.finditer(r'\\b' + re.escape(op) + r'\\b', text, re.IGNORECASE):\n",
    "                start, end = match.span()\n",
    "                entities.append((start, end, \"OPERADORA\"))\n",
    "                \n",
    "        entities.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Adiciona o texto e suas entidades à lista de dados de treinamento\n",
    "        train_data.append((text, entities))\n",
    "\n",
    "    # Escrevendo em arquivo python\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# Arquivo de treinamento para o spaCy\\n\")\n",
    "        f.write(f\"# Contém as primeiras {len(train_data)} linhas do arquivo de entrada.\\n\\n\")\n",
    "        f.write(\"train_data = [\\n\")\n",
    "        for item in train_data:\n",
    "            # Formata cada item como uma tupla\n",
    "            f.write(f\"    {repr(item)},\\n\")\n",
    "        f.write(\"]\\n\")\n",
    "\n",
    "    print(f\"Processo concluído.\")\n",
    "    print(f\"O arquivo de treinamento '{output_filename}' foi gerado com {len(train_data)} linhas.\")\n",
    "\n",
    "\n",
    "# *************************************************************\n",
    "# Downsampling com proveniência\n",
    "# *************************************************************\n",
    "def downsample_all_with_provenance(groups: dict[str, dict[str, pd.DataFrame]],\n",
    "                                   random_state: int = 42,\n",
    "                                   join: str = \"inner\"):\n",
    "\n",
    "    # descobrir o menor tamanho entre todos os dataframes\n",
    "    sizes = { (g,k): len(df) for g, d in groups.items() for k, df in d.items() }\n",
    "    if not sizes:\n",
    "        return {}, 0, pd.DataFrame()\n",
    "    min_n = min(sizes.values())\n",
    "\n",
    "    # para cada dataframe, fazer downsample para min_n (sem reposição)\n",
    "    balanced_groups: dict[str, dict[str, pd.DataFrame]] = {g: {} for g in groups}\n",
    "    sampled_frames = []\n",
    "    for g, mapping in groups.items():\n",
    "        for k, df in mapping.items():\n",
    "            \n",
    "            df_with_src = df.copy()\n",
    "            df_with_src[\"grupo\"] = g\n",
    "            df_with_src[\"operadora\"] = k\n",
    "            \n",
    "            if len(df_with_src) > min_n:\n",
    "                df_with_src = df_with_src.sample(n=min_n, replace=False, random_state=random_state)\n",
    "            balanced_groups[g][k] = df_with_src\n",
    "            sampled_frames.append(df_with_src)\n",
    "\n",
    "    # concatenar todos os dataframes amostrados\n",
    "    all_concat = pd.concat(sampled_frames, ignore_index=True, join=join)\n",
    "    return balanced_groups, min_n, all_concat\n",
    "\n",
    "\n",
    "# *************************************************************\n",
    "# Amostragem balanceada\n",
    "# *************************************************************\n",
    "def stratified_fix_split_with_test(df: pd.DataFrame,\n",
    "                                   key_cols=('grupo', 'operadora'),\n",
    "                                   per_subset_per_group=50,\n",
    "                                   random_state=42):\n",
    "    \"\"\"\n",
    "    Returns train_df, dev_df, test_df.\n",
    "    - train/dev: exactly `per_subset_per_group` rows per subgroup each (no replacement)\n",
    "    - test: everything else left over from `df`\n",
    "    \"\"\"\n",
    "    rng = random_state\n",
    "    groups = df.groupby(list(key_cols), sort=False)\n",
    "    need = 2 * per_subset_per_group\n",
    "\n",
    "    train_parts, dev_parts = [], []\n",
    "    used_idx = []\n",
    "\n",
    "    for grp_key, gdf in groups:\n",
    "        n = len(gdf)\n",
    "        if n < need:\n",
    "            raise ValueError(\n",
    "                f\"Not enough rows in subgroup {grp_key}: need {need}, have {n}.\"\n",
    "            )\n",
    "\n",
    "        gdf = gdf.sample(frac=1, random_state=rng)\n",
    "\n",
    "        train_g = gdf.iloc[:per_subset_per_group].copy()\n",
    "        dev_g   = gdf.iloc[per_subset_per_group:need].copy()\n",
    "\n",
    "        train_parts.append(train_g)\n",
    "        dev_parts.append(dev_g)\n",
    "\n",
    "        used_idx.extend(train_g.index.tolist())\n",
    "        used_idx.extend(dev_g.index.tolist())\n",
    "\n",
    "    train_df = pd.concat(train_parts, ignore_index=False)\n",
    "    dev_df   = pd.concat(dev_parts,   ignore_index=False)\n",
    "\n",
    "    # test = df original exceto os índices usados em train/dev\n",
    "    test_df = df.drop(index=list(set(used_idx)))\n",
    "\n",
    "    # shuffle adicional\n",
    "    train_df = train_df.sample(frac=1, random_state=rng+1).reset_index(drop=True)\n",
    "    dev_df   = dev_df.sample(frac=1,   random_state=rng+2).reset_index(drop=True)\n",
    "    test_df  = test_df.sample(frac=1,  random_state=rng+3).reset_index(drop=True)\n",
    "\n",
    "    return train_df, dev_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4814/3373588540.py:4: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_operadora = {op: pd.read_csv(f\"data/operadora_{op}_ptbr.csv\").drop_duplicates(\"text\") for op in operadoras}\n",
      "/tmp/ipykernel_4814/3373588540.py:5: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_geral     = {op: pd.read_csv(f\"data/{op}_geral.csv\").drop_duplicates(\"text\")           for op in operadoras}\n",
      "/tmp/ipykernel_4814/3373588540.py:5: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_geral     = {op: pd.read_csv(f\"data/{op}_geral.csv\").drop_duplicates(\"text\")           for op in operadoras}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global downsample target (rows per DF): 9271\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74168 entries, 0 to 74167\n",
      "Data columns (total 38 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   activities                            18 non-null     object \n",
      " 1   content_type                          73741 non-null  object \n",
      " 2   creation_time                         74168 non-null  object \n",
      " 3   id                                    74167 non-null  float64\n",
      " 4   is_branded_content                    74168 non-null  bool   \n",
      " 5   lang                                  74168 non-null  object \n",
      " 6   link_attachment.caption               9076 non-null   object \n",
      " 7   link_attachment.description           8217 non-null   object \n",
      " 8   link_attachment.link                  9258 non-null   object \n",
      " 9   link_attachment.name                  9336 non-null   object \n",
      " 10  match_type                            74168 non-null  object \n",
      " 11  mcl_url                               74167 non-null  object \n",
      " 12  modified_time                         74168 non-null  object \n",
      " 13  multimedia                            53976 non-null  object \n",
      " 14  post_owner.type                       74168 non-null  object \n",
      " 15  post_owner.id                         74168 non-null  object \n",
      " 16  post_owner.name                       74160 non-null  object \n",
      " 17  post_owner.username                   66802 non-null  object \n",
      " 18  shared_post_id                        542 non-null    float64\n",
      " 19  statistics.angry_count                73671 non-null  float64\n",
      " 20  statistics.care_count                 73671 non-null  float64\n",
      " 21  statistics.comment_count              74168 non-null  int64  \n",
      " 22  statistics.haha_count                 73671 non-null  float64\n",
      " 23  statistics.like_count                 73671 non-null  float64\n",
      " 24  statistics.love_count                 73671 non-null  float64\n",
      " 25  statistics.reaction_count             73677 non-null  float64\n",
      " 26  statistics.sad_count                  73671 non-null  float64\n",
      " 27  statistics.share_count                74168 non-null  int64  \n",
      " 28  statistics.views                      55666 non-null  float64\n",
      " 29  statistics.views_date_last_refreshed  55666 non-null  object \n",
      " 30  statistics.wow_count                  73671 non-null  float64\n",
      " 31  surface.type                          74168 non-null  object \n",
      " 32  surface.id                            74168 non-null  int64  \n",
      " 33  surface.name                          74160 non-null  object \n",
      " 34  surface.username                      65791 non-null  object \n",
      " 35  text                                  74165 non-null  object \n",
      " 36  grupo                                 74168 non-null  object \n",
      " 37  operadora                             74168 non-null  object \n",
      "dtypes: bool(1), float64(11), int64(3), object(23)\n",
      "memory usage: 21.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# carregando os dados de cada operadora\n",
    "operadoras = ['oi', 'tim', 'vivo', 'claro']\n",
    "\n",
    "df_operadora = {op: pd.read_csv(f\"data/operadora_{op}_ptbr.csv\").drop_duplicates(\"text\") for op in operadoras}\n",
    "df_geral     = {op: pd.read_csv(f\"data/{op}_geral.csv\").drop_duplicates(\"text\")           for op in operadoras}\n",
    "\n",
    "# realizando downsampling, balancearmento e amostragem\n",
    "groups = {\"operadora\": df_operadora, \"geral\": df_geral}\n",
    "balanced_groups, min_n, all_8_balanced = downsample_all_with_provenance(groups, random_state=42, join=\"inner\")\n",
    "print(f\"Global downsample target (rows per DF): {min_n}\")\n",
    "\n",
    "train_df, dev_df, test_df = stratified_fix_split_with_test(\n",
    "    all_8_balanced, key_cols=('grupo','operadora'), per_subset_per_group=50, random_state=42\n",
    ")\n",
    "print(train_df.shape, dev_df.shape, test_df.shape)\n",
    "\n",
    "# sanity checks\n",
    "assert len(train_df) == 400 and len(dev_df) == 400\n",
    "print(train_df.groupby(['grupo','operadora']).size())\n",
    "print(dev_df.groupby(['grupo','operadora']).size())\n",
    "print(test_df.groupby(['grupo','operadora']).size())\n",
    "\n",
    "# limpando os textos\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(clean_text)\n",
    "dev_df[\"text\"] = dev_df[\"text\"].apply(clean_text)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando como csv\n",
    "train_df.to_csv('dataset/train.csv', index=False)\n",
    "dev_df.to_csv('dataset/dev.csv', index=False)\n",
    "test_df.to_csv('dataset/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processo concluído.\n",
      "O arquivo de treinamento 'train_v2.py' foi gerado com 400 linhas.\n",
      "Processo concluído.\n",
      "O arquivo de treinamento 'dev_v2.py' foi gerado com 400 linhas.\n",
      "Processo concluído.\n",
      "O arquivo de treinamento 'test_v2.py' foi gerado com 400 linhas.\n"
     ]
    }
   ],
   "source": [
    "# criando arquivos de treinamento para spaCy, apenas primeiras 400 linhas cada\n",
    "find_tokens('dataset/train.csv', 'train_v2.py', 400)\n",
    "find_tokens('dataset/dev.csv', 'dev_v2.py', 400)\n",
    "find_tokens('dataset/test.csv', 'test_v2.py', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "from dataset.train import train_data\n",
    "from dataset.dev import dev_data\n",
    "from dataset.test import test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"pt\")  # Modelo vazio em português\n",
    "\n",
    "doc_bin_train = DocBin()\n",
    "\n",
    "for text, entities in train_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in entities:\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        if span:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents\n",
    "    doc_bin_train.add(doc)\n",
    "\n",
    "doc_bin_train.to_disk(\"train.spacy\")\n",
    "\n",
    "doc_bin_dev = DocBin()\n",
    "\n",
    "for text, entities in dev_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in entities:\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        if span:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents\n",
    "    doc_bin_dev.add(doc)\n",
    "\n",
    "doc_bin_dev.to_disk(\"dev.spacy\")\n",
    "\n",
    "#  Script para treinar o modelo spaCy usando os arquivos gerados:\n",
    "#  python3 -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Avaliação do Modelo no Dataset de Desenvolvimento ---\n",
      "\n",
      "\n",
      "Gerando o arquivo Excel com os resultados...\n",
      "✅ Processo concluído! Os resultados da avaliação foram salvos em 'model_evaluation_output_last_v2.xlsx'\n"
     ]
    }
   ],
   "source": [
    "model = \"last\"\n",
    "\n",
    "# 1. Carregar o modelo treinado que foi salvo na pasta 'output/model-best'\n",
    "nlp_ner = spacy.load(\"./output/model-\" + model)\n",
    "\n",
    "print(\"--- Avaliação do Modelo no Dataset de Desenvolvimento ---\\n\")\n",
    "\n",
    "# Lista para armazenar cada linha de resultado para o Excel\n",
    "results_for_excel = []\n",
    "\n",
    "# 2. Iterar sobre os dados de desenvolvimento para comparar o previsto com o correto\n",
    "for example in test_data:\n",
    "    text = example[0]\n",
    "    true_annotations = example[1]\n",
    "    \n",
    "    true_entities_list = sorted([text[start:end] for start, end, label in true_annotations])\n",
    "    doc = nlp_ner(text)\n",
    "    predicted_entities_list = sorted([ent.text for ent in doc.ents])\n",
    "    is_equal = (true_entities_list == predicted_entities_list)\n",
    "    \n",
    "    # Adiciona os resultados formatados à lista\n",
    "    results_for_excel.append({\n",
    "        \"text\": text,\n",
    "        \"actual_annotations\": true_annotations,\n",
    "        \"predicted_annotations\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents],\n",
    "        \"actual_entities\": \", \".join(true_entities_list),\n",
    "        \"predicted_entities\": \", \".join(predicted_entities_list),\n",
    "        \"is_equal\": is_equal,\n",
    "        \"model\": model\n",
    "    })\n",
    "\n",
    "# 3. Gerar o Arquivo Excel\n",
    "if results_for_excel:\n",
    "    output_filename = \"model_evaluation_output_\"+ model +\"_v2.xlsx\"\n",
    "    \n",
    "    print(\"\\nGerando o arquivo Excel com os resultados...\")\n",
    "\n",
    "    results_df = pd.DataFrame(results_for_excel)\n",
    "    results_df.to_excel(output_filename, index=False, engine='xlsxwriter')\n",
    "    \n",
    "    print(f\"✅ Processo concluído! Os resultados da avaliação foram salvos em '{output_filename}'\")\n",
    "else:\n",
    "    print(\"\\nNenhum dado foi processado. O arquivo Excel não foi gerado.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
